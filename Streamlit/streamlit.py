import cv2
import numpy as np
import streamlit as st
from PIL import Image
from ultralytics import YOLO
from ultralytics.utils.plotting import Annotator

import chapter3 as c3
import chapter4 as c4
import chapter9 as c9
import os
import onnxruntime as ort


class ImageProcessingApp:
    def __init__(self):
        self.st = st
        self.chapter = ""
        self.function = ""
        self.uploaded_file = None
        self.img_cv = None
        self.imgout = None

    def web_ui(self):
        """Set up the web UI for the app."""
        self.st.set_page_config(
            page_title="·ª®ng d·ª•ng x·ª≠ l√Ω ·∫£nh s·ªë", layout="wide"
        )
        # Display snow only if it's the first time the app is loaded
        if 'snow_displayed' not in self.st.session_state:
            self.st.snow()
            self.st.session_state.snow_displayed = True

        # Custom dark style and animations
        self.st.markdown("""
            <style>
                html, body, [class*="css"] {
                    background-color: #111 !important;
                    color: #eee !important;
                    font-family: 'Segoe UI', sans-serif;
                    transition: all 0.3s ease;
                }
                .stApp {
                    background-color: #111;
                    padding: 20px;
                    padding-bottom: 60px; /* Gi·ªØ kho·∫£ng tr·ªëng cho footer */
                }
                /* Title styling */
                .app-title {
                    text-align: center;
                    font-size: 2.5rem;
                    font-weight: bold;
                    color: #FF64DA; /* M√†u h·ªìng */
                    margin-bottom: 30px;
                    animation: fadeIn 1.2s ease-in-out;
                }
                /* Sidebar */
                section[data-testid="stSidebar"] {
                    background-color: #1e1e1e;
                    border-right: 1px solid #333;
                }
                /* Image fade-in + scale */
                .stImage > img {
                    border-radius: 10px;
                    box-shadow: 0 5px 20px rgba(0, 255, 204, 0.2);
                    animation: scaleIn 0.7s ease;
                    transition: transform 0.3s;
                }
                .stImage:hover > img {
                    transform: scale(1.02);
                }
                @keyframes fadeIn {
                    0% {opacity: 0; transform: translateY(-20px);}
                    100% {opacity: 1; transform: translateY(0);}
                }
                @keyframes scaleIn {
                    0% {opacity: 0; transform: scale(0.9);}
                    100% {opacity: 1; transform: scale(1);}
                }

                /* Footer */
                #custom-footer {
                    position: fixed;
                    bottom: 0;
                    width: 100%;
                    background-color: #111;
                    color: #eee;
                    text-align: center;
                    padding: 10px 0;
                    font-size: 0.9rem;
                }
            </style>
        """, unsafe_allow_html=True)

        # Ti√™u ƒë·ªÅ ch√≠nh m√†u h·ªìng
        self.st.markdown(
            '<div class="app-title">·ª®ng d·ª•ng x·ª≠ l√Ω ·∫£nh s·ªë</div>', unsafe_allow_html=True
        )

        self.st.sidebar.markdown(f"""
            <div style="padding-left: 1cm; margin-bottom: -80px; margin-top:-2cm;">
                <img src="data:image/png;base64,{self.get_base64_of_image('image/logo.png')}" 
                    style="width: 200px; border-radius: 3px;" />
            </div>
        """, unsafe_allow_html=True)

        # Footer
        self.st.markdown(
            """
            <footer id="custom-footer">
                Made with ‚ù§Ô∏è | Image Processing App | 2025
            </footer>
            """,
            unsafe_allow_html=True
        )

    def get_base64_of_image(self, path):
        import base64
        with open(path, "rb") as image_file:
            return base64.b64encode(image_file.read()).decode()

    def sidebar(self):
        with self.st.sidebar:
            self.st.header("Ch·ªçn ch∆∞∆°ng v√† ch·ª©c nƒÉng")
            # Ch·ªçn ch∆∞∆°ng
            self.chapter = self.st.selectbox("üìö Ch·ªçn ch∆∞∆°ng", [
                "",
                "Nh·∫≠n di·ªán khu√¥n m·∫∑t",
                "Nh·∫≠n di·ªán tr√°i c√¢y",
                "Chapter 3",
                "Chapter 4",
                "Chapter 9",
                "Ph√¢n bi·ªát tr√°i c√¢y t∆∞∆°i hay h·ªèng",
                "Nh·∫≠n di·ªán bi·ªÉn b√°o giao th√¥ng"
            ])

            # N·∫øu l√† nh·∫≠n di·ªán khu√¥n m·∫∑t,... kh√¥ng hi·ªÉn th·ªã g√¨ n·ªØa
            if self.chapter in ["Nh·∫≠n di·ªán khu√¥n m·∫∑t", "Ph√¢n bi·ªát tr√°i c√¢y t∆∞∆°i hay h·ªèng", "Nh·∫≠n di·ªán bi·ªÉn b√°o giao th√¥ng"]:
                return  # Kh√¥ng hi·ªÉn th·ªã th√™m b·∫•t k·ª≥ l·ª±a ch·ªçn n√†o

            # N·∫øu ng∆∞·ªùi d√πng ƒë√£ ch·ªçn ch∆∞∆°ng (ngo√†i ch∆∞∆°ng ƒë·∫∑c bi·ªát), hi·ªÉn th·ªã ch·∫ø ƒë·ªô xem k·∫øt qu·∫£
            if self.chapter:
                # N·∫øu ch·ªçn YOLO, t·ª± ƒë·ªông ch·ªçn ch·∫ø ƒë·ªô "·∫¢nh g·ªëc v√† k·∫øt qu·∫£"
                if self.chapter == "Nh·∫≠n di·ªán tr√°i c√¢y":
                    self.view_mode = "·∫¢nh g·ªëc v√† k·∫øt qu·∫£"
                else:
                    # Ch·ªçn ch·∫ø ƒë·ªô xem k·∫øt qu·∫£
                    self.view_mode = self.st.selectbox("‚öôÔ∏è Ch·ªçn ch·∫ø ƒë·ªô xem k·∫øt qu·∫£", [
                                                       "·∫¢nh g·ªëc v√† k·∫øt qu·∫£", "Comparison"])

                # Ch·ªçn ch·ª©c nƒÉng theo ch∆∞∆°ng ƒë√£ ch·ªçn
                function_list = []
                if self.chapter == "Chapter 3":
                    function_list = ["Negative", "NegativeColor", "Logarit", "Power", "PiecewiseLine",
                                     "Histogram", "HistEqual", "HistEqualColor", "LocalHist", "HistStat",
                                     "SmoothBox", "SmoothGauss", "MedianFilter", "Sharpening", "SharpeningMask", "Gradient"]
                elif self.chapter == "Chapter 4":
                    function_list = ["Spectrum", "RemoveMorieSimple", "RemoveInterference",
                                     "CreateMotion", "Demotion", "DemotionNoise"]
                elif self.chapter == "Chapter 9":
                    function_list = ["Erosion",
                                     "Dilation", "Boundary", "Contour"]
                self.function = self.st.selectbox(
                    "‚öôÔ∏è Ch·ªçn ch·ª©c nƒÉng", [""] + function_list if function_list else [])

    def upload_image(self):
        if self.chapter:
            self.uploaded_file = self.st.file_uploader(
                "üñºÔ∏è T·∫£i ·∫£nh l√™n", type=["jpg", "jpeg", "png", "bmp", "tif", "webp"], key="image_uploader"
            )

        if not self.uploaded_file:
            image_extensions = ["jpg", "jpeg", "png", "bmp", "tif", "webp"]
            found = False

            if self.function:
                for ext in image_extensions:
                    test_image_path = f"image/test/{self.function}.{ext}"
                    if os.path.exists(test_image_path):
                        self.uploaded_file = open(test_image_path, "rb")
                        found = True
                        break

            if not found:
                for ext in image_extensions:
                    test_image_path = f"image/test/{self.chapter}.{ext}"
                    if os.path.exists(test_image_path):
                        self.uploaded_file = open(test_image_path, "rb")
                        break

        if self.uploaded_file:
            img = Image.open(self.uploaded_file)
            img_array = np.array(img)
            if len(img_array.shape) == 2:
                if img_array.dtype == np.bool_:
                    img_array = img_array.astype(np.uint8) * 255
                self.img_cv = img_array
            else:
                self.img_cv = cv2.cvtColor(img_array, cv2.COLOR_RGB2BGR)

    def process_image(self):
        if self.img_cv is None:
            return

        img = self.img_cv
        self.imgout = None

        try:
            # Danh s√°ch c√°c h√†m y√™u c·∫ßu ·∫£nh m√†u v√† ·∫£nh ƒëen tr·∫Øng
            grayscale_required = {
                "Negative", "Logarit", "Power", "PiecewiseLine", "Histogram",
                "HistEqual", "LocalHist", "HistStat",
                "SmoothBox", "SmoothGauss", "MedianFilter",
                "Sharpening", "SharpeningMask",
                "Gradient", "Demotion", "DemotionNoise",
                "Spectrum", "RemoveMorie", "RemoveInterference", "CreateMotion", "Demotion",
                "Erosion", "Dilation", "Boundary", "Contour"
            }

            color_required = {
                "NegativeColor", "HistEqualColor"
            }

            # X√°c ƒë·ªãnh ·∫£nh ƒë·∫ßu v√†o c√≥ ph·∫£i grayscale kh√¥ng
            is_grayscale = len(img.shape) == 2 or (
                len(img.shape) == 3 and img.shape[2] == 1)

            # X√°c ƒë·ªãnh ·∫£nh ƒë·∫ßu v√†o c√≥ ph·∫£i ·∫£nh m√†u
            is_color = len(img.shape) == 3 and img.shape[2] == 3

            # Ki·ªÉm tra ƒëi·ªÅu ki·ªán ·∫£nh m√†u/x√°m tr∆∞·ªõc khi x·ª≠ l√Ω
            if self.function in grayscale_required and not is_grayscale:
                self.st.warning(
                    "‚ö†Ô∏è Ch·ª©c nƒÉng n√†y y√™u c·∫ßu **·∫£nh ƒëen tr·∫Øng (grayscale)**. Vui l√≤ng ch·ªçn l·∫°i ·∫£nh.")
                return
            if self.function in color_required and not is_color:
                self.st.warning(
                    "‚ö†Ô∏è Ch·ª©c nƒÉng n√†y y√™u c·∫ßu **·∫£nh m√†u (RGB)**. Vui l√≤ng ch·ªçn l·∫°i ·∫£nh.")
                return

            # X·ª≠ l√Ω theo ch∆∞∆°ng
            if self.chapter == "Chapter 3":
                func_map = {
                    "Negative": c3.Negative, "NegativeColor": c3.NegativeColor,
                    "Logarit": c3.Logarit, "Power": c3.Power,
                    "PiecewiseLine": c3.PiecewiseLine, "Histogram": c3.Histogram,
                    "HistEqual": lambda x: cv2.equalizeHist(x),
                    "HistEqualColor": c3.HistEqualColor, "LocalHist": c3.LocalHist,
                    "HistStat": c3.HistStat,
                    "SmoothBox": lambda x: cv2.boxFilter(x, cv2.CV_8UC1, (21, 21)),
                    "SmoothGauss": lambda x: cv2.GaussianBlur(x, (43, 43), 7.0),
                    "MedianFilter": lambda x: cv2.medianBlur(x, 5),
                    "Sharpening": c3.Sharpening, "SharpeningMask": c3.SharpeningMask,
                    "Gradient": c3. Gradient
                }
                if self.function in func_map:
                    self.imgout = func_map[self.function](img)

            elif self.chapter == "Chapter 4":
                if self.function == "DemotionNoise":
                    temp = cv2.medianBlur(img, 7)
                    self.imgout = c4.Demotion(temp)
                else:
                    func_map = {
                        "Spectrum": c4.Spectrum, "RemoveMorieSimple": c4.RemoveMorie,
                        "RemoveInterference": c4.RemoveInterference, "CreateMotion": c4.CreateMotion,
                        "Demotion": c4.Demotion
                    }
                    if self.function in func_map:
                        self.imgout = func_map[self.function](img)

            elif self.chapter == "Chapter 9":
                func_map = {
                    "Erosion": c9.Erosion, "Dilation": c9.Dilation,
                    "Boundary": c9.Boundary, "Contour": c9.Contour
                }
                if self.function in func_map:
                    self.imgout = func_map[self.function](img)

            elif self.chapter == "Nh·∫≠n di·ªán tr√°i c√¢y" or self.chapter:
                # Ki·ªÉm tra n·∫øu ·∫£nh kh√¥ng ph·∫£i ·∫£nh m√†u
                if not is_color:
                    self.st.warning(
                        "‚ö†Ô∏èY√™u c·∫ßu **·∫£nh m√†u (RGB)** ƒë·ªÉ ph√°t hi·ªán ƒë·ªëi t∆∞·ª£ng. Vui l√≤ng ch·ªçn ·∫£nh kh√°c.")
                    return

                self.imgout = img.copy()
                model = YOLO("model/best.onnx")
                results = model.predict(img, conf=0.5, verbose=False)
                annotator = Annotator(self.imgout)
                names = model.names
                boxes = results[0].boxes.xyxy.cpu()
                clss = results[0].boxes.cls.cpu().tolist()
                confs = results[0].boxes.conf.tolist()
                for box, cls, conf in zip(boxes, clss, confs):
                    label = f"{names[int(cls)]} {conf:.2f}"
                    annotator.box_label(box, label, color=(
                        255, 255, 255), txt_color=(255, 0, 0))
                self.imgout = annotator.result()
        except Exception as e:
            self.st.error(f"‚ùå ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω ·∫£nh: {e}")

    def display_output(self):
        if self.img_cv is not None and self.imgout is not None:
            # N·∫øu ch·ªçn ch·∫ø ƒë·ªô ·∫¢nh g·ªëc v√† k·∫øt qu·∫£
            if self.view_mode == "·∫¢nh g·ªëc v√† k·∫øt qu·∫£":
                col1, col2 = self.st.columns(2)
                with col1:
                    self.st.image(
                        cv2.cvtColor(self.img_cv, cv2.COLOR_BGR2RGB) if len(
                            self.img_cv.shape) == 3 else self.img_cv,
                        caption="üñºÔ∏è ·∫¢nh g·ªëc", use_container_width=True
                    )
                with col2:
                    self.st.image(
                        cv2.cvtColor(self.imgout, cv2.COLOR_BGR2RGB) if len(
                            self.imgout.shape) == 3 else self.imgout,
                        caption="üì∏ K·∫øt qu·∫£ x·ª≠ l√Ω", use_container_width=True
                    )
            # N·∫øu ch·ªçn ch·∫ø ƒë·ªô Comparison
            elif self.view_mode == "Comparison" and self.chapter != "Nh·∫≠n di·ªán tr√°i c√¢y":
                from streamlit_image_comparison import image_comparison

                # Chuy·ªÉn ·∫£nh t·ª´ BGR sang RGB tr∆∞·ªõc khi hi·ªÉn th·ªã trong ch·∫ø ƒë·ªô comparison
                img1_rgb = cv2.cvtColor(self.img_cv, cv2.COLOR_BGR2RGB) if len(
                    self.img_cv.shape) == 3 else self.img_cv
                img2_rgb = cv2.cvtColor(self.imgout, cv2.COLOR_BGR2RGB) if len(
                    self.imgout.shape) == 3 else self.imgout

                # Hi·ªÉn th·ªã ·∫£nh so s√°nh
                static_component = image_comparison(
                    img1=img1_rgb,
                    img2=img2_rgb,
                    label1="·∫¢nh g·ªëc",
                    label2="K·∫øt qu·∫£ x·ª≠ l√Ω",
                    width=700
                )

    def predict_fruit_freshness(self):
        session = ort.InferenceSession("model/model.onnx")

        if self.img_cv is None:
            return

        # Ti·ªÅn x·ª≠ l√Ω ·∫£nh: resize th√†nh (150, 150), chu·∫©n h√≥a v√† gi·ªØ nguy√™n ƒë·ªãnh d·∫°ng NHWC
        img = self.img_cv  # <-- Thay img b·∫±ng self.img_cv tr·ª±c ti·∫øp
        img_resized = cv2.resize(img, (150, 150))
        img_normalized = img_resized.astype(np.float32) / 255.0  # [0,1]

        # Th√™m batch dimension -> shape: [1, 150, 150, 3]
        img_input = np.expand_dims(img_normalized, axis=0)

        # Ki·ªÉm tra ki·ªÉu d·ªØ li·ªáu ƒë·∫ßu v√†o c·ªßa m√¥ h√¨nh
        input_type = session.get_inputs()[0].type
        if input_type == "tensor(float)":
            img_input = img_input.astype(np.float32)
        else:
            raise ValueError(f"Unsupported input type: {input_type}")

        # D·ª± ƒëo√°n
        input_name = session.get_inputs()[0].name
        outputs = session.run(None, {input_name: img_input})

        prediction = float(outputs[0][0][0])  # K·∫øt qu·∫£ l√† x√°c su·∫•t h·ªèng

        # Hi·ªÉn th·ªã k·∫øt qu·∫£
        col1, col2 = self.st.columns(2)
        with col1:
            self.st.image(cv2.cvtColor(self.img_cv, cv2.COLOR_BGR2RGB),
                          caption="·∫¢nh g·ªëc", use_container_width=True)

        with col2:
            if prediction > 0.5:
                self.st.error(
                    f"‚ùå K·∫øt qu·∫£: Tr√°i c√¢y **h·ªèng** ({prediction*100:.2f}%)")
            else:
                self.st.success(
                    f"‚úÖ K·∫øt qu·∫£: Tr√°i c√¢y **t∆∞∆°i** ({(1 - prediction)*100:.2f}%)")

    def run_face_detection_video(self):
        self.st.info(
            "üìπ Nh·∫•n 'Start' ƒë·ªÉ b·∫≠t webcam v√† nh·∫≠n di·ªán khu√¥n m·∫∑t t·ª´ng ng∆∞·ªùi.")
        start = self.st.button("‚ñ∂Ô∏è Start")

        if start:
            try:
                # Load m√¥ h√¨nh
                detector = cv2.FaceDetectorYN.create(
                    model="model/face_detection_yunet_2023mar.onnx",
                    config="",
                    input_size=(320, 320),
                    score_threshold=0.9,
                    nms_threshold=0.3,
                    top_k=5000,
                    backend_id=0,
                    target_id=0,
                )

                recognizer = cv2.FaceRecognizerSF.create(
                    "model/face_recognition_sface_2021dec.onnx", "")

                # Load m√¥ h√¨nh SVM v√† danh s√°ch t√™n ng∆∞·ªùi
                import joblib
                svc = joblib.load("model/svc.pkl")
                # Danh s√°ch t√™n t∆∞∆°ng ·ª©ng label
                mydict = ['DiemQuynh', 'KaPhuc', 'LamHieu', 'QuynhThu', 'ThuHang']

                # Kh·ªüi t·∫°o webcam
                cap = cv2.VideoCapture(0)
                frame_placeholder = self.st.empty()

                while cap.isOpened():
                    ret, frame = cap.read()
                    if not ret:
                        break

                    h, w, _ = frame.shape
                    detector.setInputSize((w, h))
                    _, faces = detector.detect(frame)

                    if faces is not None:
                        for face in faces:
                            coords = list(map(int, face[:4]))  # (x, y, w, h)
                            face_crop = frame[coords[1]:coords[1] +
                                              coords[3], coords[0]:coords[0]+coords[2]]

                            if face_crop.size == 0:
                                continue

                            # Align face
                            aligned_face = recognizer.alignCrop(frame, face)
                            # Extract features
                            face_feature = recognizer.feature(aligned_face)
                            # Predict with SVM
                            pred = svc.predict(face_feature)
                            name = mydict[int(pred[0])] if int(
                                pred[0]) < len(mydict) else "Unknown"

                            # V·∫Ω box v√† t√™n
                            cv2.rectangle(
                                frame, (coords[0], coords[1]), (coords[0]+coords[2], coords[1]+coords[3]), (0, 255, 0), 2)
                            cv2.putText(
                                frame, name, (coords[0], coords[1] - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 255, 0), 2)

                    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                    frame_placeholder.image(frame_rgb, channels="RGB")

                cap.release()

            except Exception as e:
                self.st.error(f"‚ùå L·ªói khi ch·∫°y webcam: {e}")

    def run_traffic_sign_detection(self):

        st.info("üö¶ Nh·∫•n v√†o Upload ·∫¢nh ho·∫∑c Video ƒë·ªÉ nh·∫≠n di·ªán bi·ªÉn b√°o giao th√¥ng.")

        model_path = "model/traffic_signs_classification.onnx"
        if not os.path.exists(model_path):
            st.error(f"‚ùå Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh t·∫°i {model_path}")
            return

        ort_session = ort.InferenceSession(model_path)

        def preprocessing(img):
            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
            img = cv2.equalizeHist(img)
            img = img / 255.0
            return img

        def getClassName(classNo):
            classNames = [
                'Speed Limit 20 km/h', 'Speed Limit 30 km/h', 'Speed Limit 50 km/h',
                'Speed Limit 60 km/h', 'Speed Limit 70 km/h', 'Speed Limit 80 km/h',
                'End of Speed Limit 80 km/h', 'Speed Limit 100 km/h', 'Speed Limit 120 km/h',
                'No passing', 'No passing for vehicles over 3.5 metric tons',
                'Right-of-way at the next intersection', 'Priority road', 'Yield',
                'Stop', 'No vehicles', 'Vehicles over 3.5 metric tons prohibited',
                'No entry', 'General caution', 'Dangerous curve to the left',
                'Dangerous curve to the right', 'Double curve', 'Bumpy road',
                'Slippery road', 'Road narrows on the right', 'Road work',
                'Traffic signals', 'Pedestrians', 'Children crossing',
                'Bicycles crossing', 'Beware of ice/snow', 'Wild animals crossing',
                'End of all speed and passing limits', 'Turn right ahead',
                'Turn left ahead', 'Ahead only', 'Go straight or right',
                'Go straight or left', 'Keep right', 'Keep left',
                'Roundabout mandatory', 'End of no passing',
                'End of no passing by vehicles over 3.5 metric tons'
            ]
            return classNames[classNo]

        def process_image(img):
            img_processed = cv2.resize(img, (32, 32))
            img_processed = preprocessing(img_processed)
            img_processed = img_processed.reshape(1, 32, 32, 1).astype(np.float32)

            inputs = {ort_session.get_inputs()[0].name: img_processed}
            predictions = ort_session.run(None, inputs)

            classIndex = np.argmax(predictions[0])
            probabilityValue = np.amax(predictions[0])

            text_class = f"CLASS: {classIndex} {getClassName(classIndex)}"
            text_prob = f"PROBABILITY: {round(probabilityValue * 100, 2)}%"

            # Resize ·∫£nh g·ªëc v·ªÅ k√≠ch th∆∞·ªõc l·ªõn h∆°n ƒë·ªÉ hi·ªÉn th·ªã r√µ r√†ng
            display_img = cv2.resize(img, (640, 480))
            cv2.putText(display_img, text_class, (20, 35), cv2.FONT_HERSHEY_SIMPLEX,
                        0.75, (0, 255, 0), 2, cv2.LINE_AA)
            cv2.putText(display_img, text_prob, (20, 75), cv2.FONT_HERSHEY_SIMPLEX,
                        0.75, (0, 255, 0), 2, cv2.LINE_AA)
            return display_img

        option = st.selectbox("Ch·ªçn ngu·ªìn h√¨nh ·∫£nh", ["Upload ·∫¢nh", "Upload Video"])

        if option == "Upload ·∫¢nh":
            uploaded_file = st.file_uploader("Ch·ªçn ·∫£nh", type=["jpg", "jpeg", "png"])
            if uploaded_file is not None:
                file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)
                img = cv2.imdecode(file_bytes, cv2.IMREAD_COLOR)
                processed_img = process_image(img)
                st.image(processed_img, channels="BGR", caption="K·∫øt qu·∫£ nh·∫≠n di·ªán")

        elif option == "Upload Video":
            uploaded_video = st.file_uploader("Ch·ªçn video", type=["mp4", "avi", "mov", "gif"])
            if uploaded_video is not None:
                # L∆∞u file video t·∫°m th·ªùi
                with open("temp_video.mp4", "wb") as f:
                    f.write(uploaded_video.read())
                
                # M·ªü video b·∫±ng OpenCV
                cap = cv2.VideoCapture("temp_video.mp4")
                
                # Ki·ªÉm tra xem video c√≥ m·ªü ƒë∆∞·ª£c kh√¥ng
                if not cap.isOpened():
                    st.error("‚ùå Kh√¥ng th·ªÉ m·ªü video. Ki·ªÉm tra ƒë·ªãnh d·∫°ng ho·∫∑c file video.")
                    return
                
                # Hi·ªÉn th·ªã video frame-by-frame
                stop_video = st.button("üõë D·ª´ng Video")
                frame_placeholder = st.empty()
                
                while not stop_video:
                    ret, frame = cap.read()
                    
                    # N·∫øu video k·∫øt th√∫c, ƒë·∫∑t l·∫°i v·ªÅ khung h√¨nh ƒë·∫ßu ti√™n
                    if not ret:
                        # ƒê·∫∑t l·∫°i video v·ªÅ khung h√¨nh ƒë·∫ßu ti√™n
                        cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
                        continue
                    
                    # X·ª≠ l√Ω khung h√¨nh
                    processed_frame = process_image(frame)
                    
                    # Hi·ªÉn th·ªã khung h√¨nh ƒë√£ x·ª≠ l√Ω
                    frame_placeholder.image(processed_frame, channels="BGR", caption="Video ƒëang x·ª≠ l√Ω")
                
                # Gi·∫£i ph√≥ng t√†i nguy√™n
                cap.release()
                os.remove("temp_video.mp4")  # X√≥a file video t·∫°m th·ªùi
                st.success("‚úÖ ƒê√£ d·ª´ng video.")

    def run(self):
        """Run the full app."""
        self.web_ui()
        self.sidebar()

        if self.chapter == "Nh·∫≠n di·ªán khu√¥n m·∫∑t":
            self.run_face_detection_video()
        elif self.chapter == "Nh·∫≠n di·ªán tr√°i c√¢y":
            self.upload_image()
            self.process_image()
            self.display_output()
        elif self.chapter == "Ph√¢n bi·ªát tr√°i c√¢y t∆∞∆°i hay h·ªèng":
            self.upload_image()
            self.predict_fruit_freshness()
        elif self.chapter == "Nh·∫≠n di·ªán bi·ªÉn b√°o giao th√¥ng":  
            self.run_traffic_sign_detection()
        else:
            if self.chapter and self.function:
                self.upload_image()
                self.process_image()
                self.display_output()


if __name__ == "__main__":
    app = ImageProcessingApp()
    app.run()
